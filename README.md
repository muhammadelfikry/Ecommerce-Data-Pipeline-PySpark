# Data Pipeline With PySpark

## Background
A data pipeline is a set of processes that move data from one or more sources (data sources) into a target database or analysis system (destination). ETL processes in a data pipeline play a role in cleaning, organizing raw data, and preparing it for storage, data analysis, and machine learning (ML). PySpark can be used to build data pipelines due to its superior ability to handle big data.

## Project Scope
This project aims to develop a data pipeline using PySpark. The ETL process will be performed by utilizing the database contained in Supabase as the source and target data. This process includes data transformation as well as RFM analysis execution.

Data source: https://www.kaggle.com/datasets/vivek468/superstore-dataset-final?select=Sample+-+Superstore.csv

## Usability
- Automate the data pipeline for analysis.
- Perform customer segmentation based on RFM analysis.
